{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chinnis18/Meeting_Summarizer/blob/main/Copy_of_Meeting_Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PapwbKustuoc"
      },
      "outputs": [],
      "source": [
        "!pip install moviepy groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
        "#define the paths\n",
        "video_path=\"/videoplayback.mp4\"\n",
        "audio_path=\"output_audio.mp3\"\n",
        "#load and covert\n",
        "clip=VideoFileClip(video_path)\n",
        "clip.audio.write_audiofile(audio_path)\n",
        "print(\"Audio Extracted:\", audio_path)"
      ],
      "metadata": {
        "id": "f9dKXOGGuKEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from groq import Groq\n",
        "os.environ[\"GROQ_API_KEY\"]=\"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\"\n",
        "#initailize the groq client\n",
        "client=Groq()\n",
        "filename=\"output_audio.mp3\"\n",
        "#open the audio file\n",
        "with open(filename,\"rb\") as file:\n",
        "  transcription = client.audio.transcriptions.create(\n",
        "      file=file,\n",
        "      model=\"whisper-large-v3-turbo\",\n",
        "      response_format=\"verbose_json\",\n",
        "      #timestamp_granularities=[\"word\",\"segment\"],\n",
        "      language=\"en\",\n",
        "      temperature=\"0.0\"\n",
        "  )\n",
        "  print(transcription.text)"
      ],
      "metadata": {
        "id": "0eZbthtWzJAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.dumps(transcription,indent=2,default=str))"
      ],
      "metadata": {
        "id": "ql6Ly5TDCHAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
        "def extract_audio(video_path:str,audio_path:str) ->str:\n",
        "  \"\"\"\n",
        "  Extracts audio from a video file and saves it as an MP3 file.\n",
        "  Args:\n",
        "    video_path: The path to the video file.\n",
        "    audio_path: The path to save the audio file.\n",
        "  Returns:\n",
        "   str: The path to the saved audio file.\n",
        "  \"\"\"\n",
        "  clip=VideoFileClip(video_path)\n",
        "  clip.audio.write_audiofile(audio_path)\n",
        "  print(\"Audio Extracted:\", audio_path)\n",
        "  return audio_path\n"
      ],
      "metadata": {
        "id": "ltlBPPm-Dnoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path=\"/videoplayback.mp4\"\n",
        "audio_path=\"output_audio.mp3\"\n",
        "saved_audio_path=extract_audio(video_path,audio_path)"
      ],
      "metadata": {
        "id": "wuwr5MTLIbN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from groq import Groq\n",
        "#load envirnoment variables\n",
        "load_dotenv()\n",
        "# GROQ_API_KEY=\"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\" # Remove explicit key assignment here\n",
        "def transcribe_audio(audio_path:str) ->str:\n",
        "  \"\"\"\n",
        "  Transcribes an audio file using the Groq API.\n",
        "  Args:\n",
        "    audio_path: The path to the audio file.\n",
        "  Returns:\n",
        "    str:The transcribed text.\n",
        "  \"\"\"\n",
        "  # client=Groq(api_key=GROQ_API_KEY) # Remove client initialization from here\n",
        "  with open(audio_path,\"rb\") as file:\n",
        "    transcription=client.audio.transcriptions.create( # Use the global client\n",
        "        file=file,\n",
        "        model=\"whisper-large-v3-turbo\",\n",
        "        prompt=\"Specify context or spelling\",\n",
        "        response_format=\"verbose_json\",\n",
        "        timestamp_granularities=[\"word\",\"segment\"],\n",
        "        language=\"en\",\n",
        "        temperature=\"0.0\"\n",
        "    )\n",
        "    print(\"Transcription completed.\")\n",
        "    return transcription.text\n",
        "transcription_text=transcribe_audio(audio_path)\n",
        "print(transcription_text)"
      ],
      "metadata": {
        "id": "GBHJx-5MGNs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/openai/whisper.git\n"
      ],
      "metadata": {
        "id": "_AhYMtBzbRjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"base\")  # or \"small\", \"medium\", \"large\"\n",
        "result = model.transcribe(audio_path) # Use the variable audio_path\n",
        "transcription_text = result[\"text\"]\n",
        "print(transcription_text)"
      ],
      "metadata": {
        "id": "cTnej-1dbVYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers\n"
      ],
      "metadata": {
        "id": "utYypQybb-uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load summarization pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "def summarize_meeting(transcript):\n",
        "    chunks = [transcript[i:i+1000] for i in range(0, len(transcript), 1000)]\n",
        "    summaries = [summarizer(chunk, max_length=150, min_length=40, do_sample=False)[0]['summary_text'] for chunk in chunks]\n",
        "    return \"\\n\".join(summaries)\n",
        "\n",
        "summary_text = summarize_meeting(transcription_text)\n",
        "print(\"üìù Summary:\\n\", summary_text)\n"
      ],
      "metadata": {
        "id": "gHXXI7xEcGYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_action_items(summary):\n",
        "    lines = summary.split(\".\")\n",
        "    actions = [line.strip() for line in lines if any(kw in line.lower() for kw in [\"will\", \"should\", \"must\", \"decided\", \"agreed\", \"assigned\"])]\n",
        "    return actions\n",
        "\n",
        "action_items = extract_action_items(summary_text)\n",
        "print(\"üìå Action Items:\")\n",
        "for item in action_items:\n",
        "    print(\"-\", item)\n"
      ],
      "metadata": {
        "id": "zJVgu-dBcqa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
        "import whisper # Corrected import\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load models\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "def process_file(file):\n",
        "    # Extract audio\n",
        "    clip = VideoFileClip(file)\n",
        "    audio_path = \"temp_audio.mp3\"\n",
        "    clip.audio.write_audiofile(audio_path, logger=None)\n",
        "\n",
        "    # Transcribe\n",
        "    result = whisper_model.transcribe(audio_path)\n",
        "    transcription_text = result[\"text\"]\n",
        "\n",
        "    # Summarize\n",
        "    chunks = [transcription_text[i:i+1000] for i in range(0, len(transcription_text), 1000)]\n",
        "    summaries = [summarizer(chunk, max_length=150, min_length=40, do_sample=False)[0]['summary_text'] for chunk in chunks]\n",
        "    summary_text = \"\\n\".join(summaries)\n",
        "\n",
        "    # Extract action items\n",
        "    lines = summary_text.split(\".\")\n",
        "    actions = [line.strip() for line in lines if any(kw in line.lower() for kw in [\"will\", \"should\", \"must\", \"decided\", \"agreed\", \"assigned\"])]\n",
        "\n",
        "    return transcription_text, summary_text, \"\\n\".join(actions)\n",
        "\n",
        "# Gradio Interface\n",
        "demo = gr.Interface(\n",
        "    fn=process_file,\n",
        "    inputs=gr.File(type=\"filepath\", label=\"Upload Audio/Video\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Transcription\", lines=15, max_lines=30),\n",
        "        gr.Textbox(label=\"Summary\", lines=10, max_lines=20),\n",
        "        gr.Textbox(label=\"Action Items\", lines=10, max_lines=20),\n",
        "    ],\n",
        "    title=\"üé• Meeting Summarizer\",\n",
        "    description=\"Upload a video/audio file ‚Üí Get transcript, summary & action items\"\n",
        ")\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "Xr50_y2nVYN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_action_items(summary):\n",
        "    lines = summary.split(\".\")\n",
        "    actions = [line.strip() for line in lines if any(kw in line.lower() for kw in [\"will\", \"should\", \"must\", \"decided\", \"agreed\", \"assigned\"])]\n",
        "    return actions\n",
        "\n",
        "action_items = extract_action_items(summary_text)\n",
        "\n",
        "print(\"üìå Action Items:\")\n",
        "for item in action_items:\n",
        "    print(\"-\", item)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# ‚úÖ Add transcription accuracy\n",
        "# ---------------------------\n",
        "!pip install jiwer\n",
        "\n",
        "from jiwer import wer\n",
        "\n",
        "def transcription_accuracy(reference_text, predicted_text):\n",
        "    \"\"\"\n",
        "    Calculates transcription accuracy using Word Error Rate (WER).\n",
        "    Args:\n",
        "        reference_text (str): Ground truth transcript\n",
        "        predicted_text (str): Model transcription\n",
        "    Returns:\n",
        "        float: accuracy percentage\n",
        "    \"\"\"\n",
        "    error = wer(reference_text, predicted_text)\n",
        "    accuracy = (1 - error) * 100\n",
        "    return accuracy\n",
        "\n",
        "# ‚úçÔ∏è Provide the TRUE transcript of your video here\n",
        "reference_text = \"Hello everyone, thank you guys for coming to our weekly student success meeting. And let's just get started. So I have our list of chronically absent students here and I've been noticing a troubling trend. A lot of students are skipping on Fridays. Does anyone have any idea what's going on? I've heard some of my mentees talking about how it's really hard to get out of bed on Fridays. It might be good if we did something like a pancake breakfast to encourage them to come. I think that's a great idea. Let's try that next week. It might also be because a lot of students have been getting sick now that it's getting colder outside. I've had a number of students come by my office with symptoms like sniffling and coughing. We should put up posters with tips for not getting sick since it's almost flu season. Like, you know, wash your hands after the bathroom. Stuff like that. I think that's a good idea and it'll be a good reminder for the teachers as well. So one other thing I wanted to talk about, there's a student I've noticed here, John Smith. He's missed seven days already and it's only November. Does anyone have an idea what's going on with him? I might be able to fill in the gaps there. I talked to John today and he's really stressed out. He's been dealing with helping his parents take care of his younger siblings during the day. It might actually be a good idea if he spoke to the guidance counselor a little bit. I can talk to John today if you want to send him to my office after you meet with him. It's a lot to deal with for middle schooler. Great thanks and I can help out with the family's childcare needs. I'll look for some free or low-cost resources in the community to share with John and he can share them with his family. Great, well some really good ideas here today. Thanks for coming and if no one has anything else I think we can wrap up.\"\n",
        "\n",
        "predicted_text = transcription_text  # from Whisper/Groq\n",
        "\n",
        "accuracy = transcription_accuracy(reference_text, predicted_text)\n",
        "print(f\"üéØ Transcription Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "cBC1niiFqA9n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}